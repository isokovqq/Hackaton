```# ===========================================
# OPTIMIZED PD PIPELINE — USE ALL FEATURES
# ===========================================
# Requirements: pandas, numpy, scikit-learn, imbalanced-learn, matplotlib, seaborn, joblib
# Install if needed: pip install imbalanced-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import warnings
warnings.filterwarnings("ignore")
plt.style.use("seaborn-v0_8-darkgrid")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (
    roc_auc_score, average_precision_score, brier_score_loss,
    precision_recall_curve, precision_score, recall_score,
    f1_score, confusion_matrix, classification_report, roc_curve
)
from imblearn.combine import SMOTEENN
from imblearn.over_sampling import SMOTE

RANDOM_STATE = 42

# ===========================================
# 1) LOAD DATA
# ===========================================
file_path = r"C:\Users\furqa\OneDrive\Desktop\python\merged_customer_data.csv"
df = pd.read_csv(file_path)

if "default" not in df.columns:
    raise ValueError("Dataset MUST contain a 'default' column.")

print("Initial rows:", df.shape[0])
df = df.dropna()
print("After dropna rows:", df.shape[0])

# ===========================================
# 2) PREPROCESSING (USE ALL FEATURES)
# ===========================================
X = df.drop("default", axis=1)
y = df["default"].astype(int)

# Label encoding categorical variables
cat_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()
label_encoders = {}
for col in cat_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    label_encoders[col] = le

print("Categorical encoded:", cat_cols)

feature_names = X.columns.tolist()

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ===========================================
# 3) TRAIN/TEST SPLIT
# ===========================================
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE
)

print("Train:", X_train.shape, "Test:", X_test.shape)
print("Train positive ratio:", y_train.mean())

# ===========================================
# 4) HANDLE IMBALANCE (ORIGINAL, SMOTE, SMOTEENN)
# ===========================================
smoteenn = SMOTEENN(random_state=RANDOM_STATE)
X_train_comb, y_train_comb = smoteenn.fit_resample(X_train, y_train)

sm = SMOTE(random_state=RANDOM_STATE)
X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)

train_sets = {
    "original": (X_train, y_train),
    "smote": (X_train_sm, y_train_sm),
    "smoteenn": (X_train_comb, y_train_comb),
}

# ===========================================
# 5) MODELS TO TRAIN
# ===========================================
models = {
    "RandomForest": RandomForestClassifier(
        n_estimators=300, max_depth=12, min_samples_leaf=3,
        class_weight="balanced", random_state=RANDOM_STATE, n_jobs=-1
    ),
    "GradientBoosting": GradientBoostingClassifier(
        n_estimators=200, max_depth=4, learning_rate=0.05, random_state=RANDOM_STATE
    )
}

results = {}

# ===========================================
# 6) TRAIN MODELS & EVALUATE
# ===========================================
for dname, (Xt, yt) in train_sets.items():
    print(f"\n--- TRAIN SET: {dname} ---")
    results[dname] = {}

    for mname, model in models.items():
        print(f"Training: {mname} on {dname}")
        model.fit(Xt, yt)

        # probabilities
        y_prob = model.predict_proba(X_test)[:, 1]

        auc = roc_auc_score(y_test, y_prob)
        ap = average_precision_score(y_test, y_prob)
        brier = brier_score_loss(y_test, y_prob)

        results[dname][mname] = {
            "model": model,
            "y_prob": y_prob,
            "auc": auc,
            "ap": ap,
            "brier": brier
        }

        print(f"  -> AUC={auc:.4f}  AP={ap:.4f}  Brier={brier:.6f}")

# ===========================================
# 7) SELECT BEST MODEL (AUC + 0.5*AP)
# ===========================================
best = None
best_score = -999

for dname, models_dict in results.items():
    for mname, info in models_dict.items():
        score = info["auc"] + 0.5 * info["ap"]
        if score > best_score:
            best_score = score
            best = (dname, mname, info)

best_train_name, best_model_name, best_info = best
best_model = best_info["model"]
y_prob_test = best_info["y_prob"]

print("\nBest model:", best_model_name, "| trained on:", best_train_name)
print("Composite score:", best_score)

# ===========================================
# 8) THRESHOLD TUNING
# ===========================================
precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob_test)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-12)

# max F1
ix_max_f1 = np.nanargmax(f1_scores)
thresh_max_f1 = thresholds[ix_max_f1 - 1] if ix_max_f1 > 0 else 0.5

# target recall method
target_recall = 0.6
idxs = np.where(recalls >= target_recall)[0]
thr_rec = None
if len(idxs) > 1:
    thr_idx = idxs[1] - 1
    thr_rec = thresholds[thr_idx]

if thr_rec is None:
    thr_rec = np.percentile(y_prob_test, 70)

print(f"\nThreshold max F1: {thresh_max_f1:.4f}")
print(f"Threshold for recall≈{target_recall}: {thr_rec:.4f}")

# ===========================================
# 9) EVALUATE THRESHOLDS
# ===========================================
thresholds_to_eval = [0.5, float(thresh_max_f1), float(thr_rec)]

for thr in thresholds_to_eval:
    yp = (y_prob_test >= thr).astype(int)
    prec = precision_score(y_test, yp)
    rec = recall_score(y_test, yp)
    f1v = f1_score(y_test, yp)
    cm = confusion_matrix(y_test, yp)
    print(f"\nThreshold {thr:.4f}:")
    print(f" Precision={prec:.3f}  Recall={rec:.3f}  F1={f1v:.3f}")
    print(" Confusion Matrix:", tuple(cm.ravel()))

# ===========================================
# 10) FINAL METRICS
# ===========================================
print("\n=== FINAL METRICS ===")
print("AUC:", best_info["auc"])
print("PR-AUC:", best_info["ap"])
print("Brier score:", best_info["brier"])

print("\nClassification report (threshold = max F1):")
y_pred_final = (y_prob_test >= thresh_max_f1).astype(int)
print(classification_report(y_test, y_pred_final))

# ===========================================
# 11) PLOTS (ROC, PR, OBSERVED vs PREDICTED)
# ===========================================

# ROC
fpr, tpr, _ = roc_curve(y_test, y_prob_test)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, label=f"{best_model_name} (AUC={best_info['auc']:.3f})")
plt.plot([0,1],[0,1],'k--')
plt.title("ROC Curve")
plt.savefig("roc_curve.png", dpi=300)
plt.show()

# PR curve
prec, rec, _ = precision_recall_curve(y_test, y_prob_test)
plt.figure(figsize=(8,6))
plt.plot(rec, prec, label="PR-AUC=%.3f" % best_info["ap"])
plt.title("Precision-Recall Curve")
plt.savefig("pr_curve.png", dpi=300)
plt.show()

# Observed vs Predicted
dfb = pd.DataFrame({"prob": y_prob_test, "actual": y_test})
dfb["bin"] = pd.cut(dfb["prob"], bins=30)
bins = dfb.groupby("bin").agg(
    prob_mean=("prob","mean"),
    obs_rate=("actual","mean"),
    count=("actual","size")
)

plt.figure(figsize=(8,6))
plt.scatter(bins["prob_mean"], bins["obs_rate"], s=bins["count"]*2, alpha=0.7)
plt.plot([0,1],[0,1],'k--')
plt.title("Observed vs Predicted")
plt.savefig("obs_vs_pred.png", dpi=300)
plt.show()

# ===========================================
# 12) FEATURE IMPORTANCE
# ===========================================
if hasattr(best_model, "feature_importances_"):
    fi = pd.DataFrame({
        "feature": feature_names,
        "importance": best_model.feature_importances_
    }).sort_values("importance", ascending=False)

    fi["importance_norm"] = fi["importance"] / fi["importance"].sum()
    print("\nTop features:\n", fi.head(20))
    fi.to_csv("feature_importance.csv", index=False)

    plt.figure(figsize=(10,6))
    plt.barh(fi["feature"].head(20)[::-1], fi["importance_norm"].head(20)[::-1])
    plt.title("Feature Importance")
    plt.savefig("feature_importance_plot.png", dpi=300)
    plt.show()

# ===========================================
# 13) SAVE MODEL + ARTIFACTS
# ===========================================
output_model = f"best_model_{best_model_name}_{best_train_name}.pkl"
joblib.dump(best_model, output_model)
joblib.dump(scaler, "final_scaler.pkl")
joblib.dump(label_encoders, "final_label_encoders.pkl")

pd.Series({
    "threshold_max_f1": float(thresh_max_f1),
    "threshold_recall_target": float(thr_rec),
    "auc": float(best_info["auc"]),
    "pr_auc": float(best_info["ap"])
}).to_json("model_meta.json")

print("\nSaved model:", output_model)

# ===========================================
# 14) PREDICTION FUNCTION
# ===========================================
def predict_pd(new_df):
    """
    Input: DataFrame with all original columns except 'default'
    Output: (probability, prediction)
    """
    model = joblib.load(output_model)
    sc = joblib.load("final_scaler.pkl")
    encs = joblib.load("final_label_encoders.pkl")

    Xn = new_df.copy()

    for col, le in encs.items():
        if col in Xn.columns:
            Xn[col] = le.transform(Xn[col].astype(str))

    Xn_scaled = sc.transform(Xn[feature_names])
    probs = model.predict_proba(Xn_scaled)[:, 1]
    preds = (probs >= float(thresh_max_f1)).astype(int)
    return probs, preds


print("\n=== PIPELINE COMPLETE ===")
```
