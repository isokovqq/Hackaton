import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.feature_selection import SelectKBest, f_classif
from catboost import CatBoostClassifier, Pool
from category_encoders import TargetEncoder

# Load data
df = pd.read_csv('data/merged_customer_data.csv')

# Drop irrelevant/noisy columns
drop_cols = ['customer_id', 'application_id', 'referral_code', 'random_noise_1', 'previous_zip_code']
df = df.drop(drop_cols, axis=1)

# Feature engineering: Group rare categories and drop low-importance features
for col in ['state', 'marketing_campaign']:
    if col in df.columns:
        counts = df[col].value_counts()
        rare = counts[counts < 100].index
        df[col] = df[col].apply(lambda x: 'Other' if x in rare else x)
if 'num_credit_accounts_0' in df.columns and df['num_credit_accounts_0'].nunique() == 1:
    df = df.drop('num_credit_accounts_0', axis=1)

# Check for multicollinearity
debt_cols = ['debt_to_income_ratio', 'total_debt_amount', 'monthly_debt_payment', 'debt_service_ratio']
for col1 in debt_cols:
    for col2 in debt_cols:
        if col1 < col2 and col1 in df.columns and col2 in df.columns:
            corr = df[[col1, col2]].corr().iloc[0, 1]
            if abs(corr) > 0.8:
                df = df.drop(col2, axis=1)

# Separate features and target
X = df.drop('default', axis=1)
y = df['default']

# Identify categorical and numeric columns
object_cols = [col for col in X.columns if X[col].dtype == 'object']
cat_cols = object_cols + [col for col in X.columns if X[col].nunique() < 50 and X[col].dtype != 'object']
num_cols = [col for col in X.columns if col not in cat_cols]

# Special scaling for credit_score
credit_score_col = 'credit_score' if 'credit_score' in num_cols else None
other_num_cols = [col for col in num_cols if col != credit_score_col]

# Preprocessing pipeline
transformers = [
    ('num', StandardScaler(), other_num_cols),
    ('cat', TargetEncoder(), cat_cols)
]
if credit_score_col:
    transformers.insert(0, ('credit_score', MinMaxScaler(), [credit_score_col]))

preprocessor = ColumnTransformer(transformers=transformers)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Apply preprocessing
X_train_preprocessed = preprocessor.fit_transform(X_train, y_train)
X_test_preprocessed = preprocessor.transform(X_test)

# Feature selection (top 30 features)
selector = SelectKBest(score_func=f_classif, k=30)
X_train_selected = selector.fit_transform(X_train_preprocessed, y_train)
X_test_selected = selector.transform(X_test_preprocessed)

# Get selected feature names
feature_names = preprocessor.get_feature_names_out()
selected_features = feature_names[selector.get_support()]

# CatBoost with focal loss
model = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.03,
    depth=6,
    l2_leaf_reg=5,
    auto_class_weights='Balanced',
    loss_function='Logloss',
    random_seed=42,
    eval_metric='F1',
    verbose=100,
    early_stopping_rounds=100
)

# Grid search for hyperparameter tuning
param_grid = {
    'depth': [4, 6],
    'learning_rate': [0.01, 0.03],
    'l2_leaf_reg': [5, 10]
}
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring='f1',
    cv=3,
    verbose=1,
    n_jobs=-1
)
grid_search.fit(X_train_selected, y_train)

# Best model
model = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# Train model
train_pool = Pool(X_train_selected, y_train)
test_pool = Pool(X_test_selected, y_test)
model.fit(train_pool)

# Predictions
y_pred = model.predict(test_pool)
y_prob = model.predict_proba(test_pool)[:, 1]

# Metrics at default threshold
print("Default Threshold (0.5):")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1:", f1_score(y_test, y_pred))
print("AUC:", roc_auc_score(y_test, y_prob))

# Optimize threshold for F1
thresholds = np.arange(0.05, 0.95, 0.05)
best_f1 = 0
best_threshold = 0.5
for thresh in thresholds:
    y_pred_thresh = (y_prob > thresh).astype(int)
    f1 = f1_score(y_test, y_pred_thresh)
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = thresh

# Metrics with best threshold
y_pred_best = (y_prob > best_threshold).astype(int)
print(f"\nBest Threshold ({best_threshold}):")
print("Accuracy:", accuracy_score(y_test, y_pred_best))
print("Precision:", precision_score(y_test, y_pred_best))
print("Recall:", recall_score(y_test, y_pred_best))
print("F1:", f1_score(y_test, y_pred_best))
print("AUC:", roc_auc_score(y_test, y_prob))

# Feature importance
importances = model.get_feature_importance()
for feat, imp in zip(selected_features, importances):
    print(f"Feature: {feat}, Importance: {imp}")
