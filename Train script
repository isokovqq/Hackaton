import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.calibration import CalibratedClassifierCV
from catboost import CatBoostClassifier, Pool
import joblib

DATA_PATH = r"C:\Users\furqa\OneDrive\Desktop\python\merged_customer_data.csv"
RESULTS_DIR = "model_results"
os.makedirs(RESULTS_DIR, exist_ok=True)

RANDOM_STATE = 42
TEST_SIZE = 0.20
TOP_K_FEATURES = 25
LIGHT_MODEL_ITERS = 400
FINAL_ITERS = 2000
CV_FOLDS = 3

def cap_outliers(s):
    q1 = s.quantile(0.25)
    q3 = s.quantile(0.75)
    iqr = q3 - q1
    return s.clip(q1 - 1.5 * iqr, q3 + 1.5 * iqr)

def compute_metrics(y_true, y_pred, y_prob):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return {
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred, zero_division=0),
        "recall": recall_score(y_true, y_pred, zero_division=0),
        "specificity": tn / (tn + fp) if (tn + fp) else 0,
        "f1": f1_score(y_true, y_pred, zero_division=0),
        "auc": roc_auc_score(y_true, y_prob),
        "tn": tn, "fp": fp, "fn": fn, "tp": tp}

df = pd.read_csv(DATA_PATH)
print("Loaded:", df.shape)

drop_cols = ['customer_id', 'application_id', 'referral_code', 'random_noise_1', 'previous_zip_code']
df = df.drop(columns=[c for c in drop_cols if c in df.columns])

if 'default' not in df.columns:
    raise ValueError("No 'default' column found.")

df['default'] = df['default'].astype(int)

for col in ['state', 'marketing_campaign']:
    if col in df.columns:
        vc = df[col].value_counts()
        rare = vc[vc < 100].index
        df[col] = df[col].apply(lambda x: 'Other' if x in rare else x)

for col in ['annual_income', 'total_debt_amount', 'loan_amount']:
    if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
        df[col] = cap_outliers(df[col])

if 'credit_score' in df.columns and 'debt_to_income_ratio' in df.columns:
    df['credit_score_debt_interaction'] = df['credit_score'] * df['debt_to_income_ratio']

if 'num_delinquencies_2yrs' in df.columns and 'credit_utilization' in df.columns:
    df['delinq_credit_util_interaction'] = df['num_delinquencies_2yrs'] * df['credit_utilization']

cat_cols = [c for c in df.columns if df[c].dtype == 'object' and c != 'default']
num_cols = [c for c in df.columns if c not in cat_cols + ['default']]

for c in num_cols.copy():
    if df[c].nunique() <= 15:
        df[c] = df[c].astype(str)
        cat_cols.append(c)
        num_cols.remove(c)

print(f"Numeric: {len(num_cols)}, Categorical: {len(cat_cols)}")

X = df.drop(columns=['default'])
y = df['default']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE)

if num_cols:
    imputer = SimpleImputer(strategy='median')
    X_train[num_cols] = imputer.fit_transform(X_train[num_cols])
    X_test[num_cols] = imputer.transform(X_test[num_cols])
else:
    imputer = None

for c in cat_cols:
    X_train[c] = X_train[c].astype(str).fillna("Missing")
    X_test[c] = X_test[c].astype(str).fillna("Missing")

print("\nTraining light CatBoost for feature selection…")

light_model = CatBoostClassifier(
    iterations=LIGHT_MODEL_ITERS,
    learning_rate=0.05,
    depth=4,
    auto_class_weights="Balanced",
    random_seed=RANDOM_STATE,
    verbose=0)

light_pool = Pool(X_train, y_train, cat_features=cat_cols)
light_model.fit(light_pool)

importances = light_model.get_feature_importance()
feature_names = list(X_train.columns)

imp_df = pd.DataFrame({"feature": feature_names, "importance": importances})
imp_df = imp_df.sort_values("importance", ascending=False).reset_index(drop=True)

selected_features = imp_df["feature"].iloc[:TOP_K_FEATURES].tolist()
print("Selected features:", selected_features)

# TRAIN/TEST 
X_train_sel = X_train[selected_features].copy()
X_test_sel = X_test[selected_features].copy()

selected_cat = [c for c in selected_features if c in cat_cols]

print("\nRunning GridSearchCV…")

base_model = CatBoostClassifier(
    iterations=1200,
    loss_function="Logloss",
    eval_metric="AUC",
    auto_class_weights="Balanced",
    random_seed=RANDOM_STATE,
    verbose=0)

param_grid = {
    "depth": [5, 6],
    "learning_rate": [0.01, 0.03],
    "l2_leaf_reg": [3, 7],}

cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)

grid = GridSearchCV(
    base_model,
    param_grid,
    scoring="f1",
    cv=cv,
    n_jobs=-1,
    verbose=2)

grid.fit(X_train_sel, y_train, cat_features=selected_cat)

print("Best Params:", grid.best_params_)
print("Best CV F1:", grid.best_score_)

print("\nTraining final model with early stopping…")

final_model = CatBoostClassifier(
    iterations=FINAL_ITERS,
    depth=grid.best_params_["depth"],
    learning_rate=grid.best_params_["learning_rate"],
    l2_leaf_reg=grid.best_params_["l2_leaf_reg"],
    loss_function="Logloss",
    eval_metric="AUC",
    auto_class_weights="Balanced",
    random_seed=RANDOM_STATE,
    verbose=100,
    early_stopping_rounds=100)

final_model.fit(
    X_train_sel, y_train,
    eval_set=(X_test_sel, y_test),
    cat_features=selected_cat,
    use_best_model=True)


print("\nCalibrating probabilities with Isotonic Regression…")

calibrator = CalibratedClassifierCV(final_model, method="isotonic", cv="prefit")
calibrator.fit(X_test_sel, y_test)

# calibrated model
final_model_calibrated = calibrator

y_prob = final_model_calibrated.predict_proba(X_test_sel)[:, 1]
y_pred_default = (y_prob >= 0.5).astype(int)

metrics_05 = compute_metrics(y_test, y_pred_default, y_prob)
print("\nMetrics @ default threshold 0.5:")
for k, v in metrics_05.items():
    if k in ["tn", "fp", "fn", "tp"]:
        continue
    print(f"{k}: {v:.4f}")
print("Confusion:", metrics_05["tn"], metrics_05["fp"], metrics_05["fn"], metrics_05["tp"])

# THRESHOLD OPTIMIZATION 
thresholds = np.arange(0.01, 0.99, 0.01)
records = []

for t in thresholds:
    yp = (y_prob >= t).astype(int)
    m = compute_metrics(y_test, yp, y_prob)
    youden = m["recall"] + m["specificity"] - 1
    bal_acc = 0.5 * (m["recall"] + m["specificity"])
    records.append({
        "threshold": t,
        "f1": m["f1"],
        "precision": m["precision"],
        "recall": m["recall"],
        "specificity": m["specificity"],
        "accuracy": m["accuracy"],
        "balanced_accuracy": bal_acc,
        "youden": youden,})

rec_df = pd.DataFrame(records)

best_f1 = rec_df.loc[rec_df["f1"].idxmax()]
best_youden = rec_df.loc[rec_df["youden"].idxmax()]
best_balacc = rec_df.loc[rec_df["balanced_accuracy"].idxmax()]

print("\nBest thresholds:")
print("Best F1:", best_f1.to_dict())
print("Best Youden:", best_youden.to_dict())
print("Best Balanced Accuracy:", best_balacc.to_dict())

important_thresholds = {
    "best_f1": best_f1.to_dict(),
    "best_youden": best_youden.to_dict(),
    "best_balacc": best_balacc.to_dict(),}

artifacts = {
    "selected_features": selected_features,
    "selected_cat": selected_cat,
    "imputer": imputer,
    "final_model_calibrated": final_model_calibrated,
    "thresholds": important_thresholds,}

joblib.dump(artifacts, os.path.join(RESULTS_DIR, "final_pd_model.joblib"))
rec_df.to_csv(os.path.join(RESULTS_DIR, "threshold_metrics.csv"), index=False)

print("\nModel + artifacts saved!")
print("Path:", os.path.join(RESULTS_DIR, "final_pd_model.joblib"))

